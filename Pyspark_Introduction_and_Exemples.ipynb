{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pyspark - Introduction and Exemples",
      "provenance": [],
      "authorship_tag": "ABX9TyMhdbaHVAnRYCfp4DHSMmGL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpforol/Studies/blob/main/Pyspark_Introduction_and_Exemples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKnru_yjc7N-",
        "outputId": "2c5d762d-43d8-4bbf-de0d-256806b15037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# innstall java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install Pyspark\n",
        "!pip3 install pyspark==3.0.2\n",
        "\n",
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "# install findspark using pip\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "0-6cK5_xdGUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "list = [1,2,3,4,5]\n",
        "rdd1 = spark.sparkContext.parallelize(list)"
      ],
      "metadata": {
        "id": "5uEQbfTGdyH-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1"
      ],
      "metadata": {
        "id": "yk60Qpdld0O1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f74cb37-c7db-4bdd-e8a0-7b9e4b3c1aca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2 = spark.sparkContext.textFile(\"/opt/spark/README.md\")\n",
        "rdd2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9CB8tk3hif7",
        "outputId": "2888300c-8a6d-44a5-e50d-461e8ec62cc0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/opt/spark/README.md MapPartitionsRDD[2] at textFile at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rddf = rdd2.flatMap(lambda line:line.split())\n",
        "rddf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72b52Cyyhou0",
        "outputId": "b944309e-2652-42ae-a778-5e56a06ed33d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[3] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Média por nome das pessoas usando a API de RDDs\n",
        "dataRDD = spark.sparkContext.parallelize([(\"Pedro\", 38), (\"Maria\", 20), (\"Pedro\",40), (\"Rafael\", 10)])\n",
        "\n",
        "agesRDD = (\n",
        "    dataRDD\n",
        "    .map(lambda x: (x[0], (x[1], 1)))\n",
        "    .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\n",
        "    .map(lambda x: (x[0], x[1][0]/x[1][1]))\n",
        ")\n",
        "agesRDD.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C0YH0UQhv08",
        "outputId": "651f27a5-c605-4b26-85a5-011b064d1221"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Pedro', 39.0), ('Maria', 20.0), ('Rafael', 10.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Média por nome das pessoas usando a API DataFrame\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "spark = (SparkSession.builder.appName(\"Ages\").getOrCreate())\n",
        "\n",
        "#Create dataframe\n",
        "data_df = spark.createDataFrame([(\"Pedro\", 38), (\"Maria\", 20), (\"Pedro\",40), (\"Rafael\", 10)], [\"nome\", \"idade\"])\n",
        "\n",
        "avg_df = data_df.groupBy(\"nome\").agg(avg(\"idade\"))\n",
        "\n",
        "avg_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-rkjPsYka6M",
        "outputId": "8364b9bb-fd69-4c53-d902-efa17e70e7cd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+\n",
            "|  nome|avg(idade)|\n",
            "+------+----------+\n",
            "| Pedro|      39.0|\n",
            "| Maria|      20.0|\n",
            "|Rafael|      10.0|\n",
            "+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.functions import rand, randn\n",
        "\n",
        "spark = SparkSession.builder.appName(\"PythonWordCount\").getOrCreate()\n",
        "\n",
        "sqlc = SQLContext(spark.sparkContext)\n",
        "\n",
        "df = (sqlc.range(0, 1000*1000)\n",
        "        .withColumn('uniform', rand(seed=10))\n",
        "        .withColumn('normal', randn(seed=27))\n",
        ")\n",
        "\n",
        "print ('# rows: ', df.count())\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjP7Pxrhk8X0",
        "outputId": "89dd436c-b2ba-40b3-ff4f-0c0d2889c97b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# rows:  1000000\n",
            "+---+-------------------+--------------------+\n",
            "| id|            uniform|              normal|\n",
            "+---+-------------------+--------------------+\n",
            "|  0| 0.1709497137955568| -0.8664700627108758|\n",
            "|  1| 0.8051143958005459| -0.5970491018333267|\n",
            "|  2| 0.5775925576589018| 0.18267161219540898|\n",
            "|  3| 0.9476047869880925| -1.8497305679917546|\n",
            "|  4|    0.2093704977577|  0.9410417279045351|\n",
            "|  5|0.36664222617947817| -0.6516475674670159|\n",
            "|  6| 0.8078688178371882|  0.5901002135239671|\n",
            "|  7| 0.7135143433452461|  -1.850241871360443|\n",
            "|  8| 0.7195325566306053| 0.09176896733073023|\n",
            "|  9|0.31335292311175456|-0.38605118617831075|\n",
            "| 10| 0.8062503712025726|  1.2134544166783332|\n",
            "| 11|0.10814914646176654| -1.0757702531630617|\n",
            "| 12| 0.3362232980701172| 0.04961226872064977|\n",
            "| 13| 0.8133304803837667|  -0.768259602441542|\n",
            "| 14|0.47649428738170896|  0.2911293146907403|\n",
            "| 15|  0.524728096293865|-0.33406080411047484|\n",
            "| 16| 0.9701253460019921|  1.3607097640771781|\n",
            "| 17| 0.6232167713919952|  0.5986772981082732|\n",
            "| 18| 0.5089687568245219|-0.35158579838711623|\n",
            "| 19| 0.5467504094508642| -0.9115825072509143|\n",
            "+---+-------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Médica, desvio padrão, valores minimo e máximo das colunas\n",
        "df.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Alt_DYpmQ0L",
        "outputId": "a4ea8922-b002-4908-cf3f-47cc53565b0f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+--------------------+--------------------+\n",
            "|summary|                id|             uniform|              normal|\n",
            "+-------+------------------+--------------------+--------------------+\n",
            "|  count|           1000000|             1000000|             1000000|\n",
            "|   mean|          499999.5|  0.5002526365480662|0.001079181935319...|\n",
            "| stddev|288675.27893234405| 0.28892225121213544|  0.9991903695324608|\n",
            "|    min|                 0|2.710561290975022E-7|  -4.955719833927487|\n",
            "|    max|            999999|  0.9999999832771045|   4.626848789533737|\n",
            "+-------+------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Computar a correlação estatística entre variáveis\n",
        "df = sqlc.range(0, 1000*1000).withColumn('rand1', rand(seed=10)).withColumn('rand2', rand(seed=27))\n",
        "\n",
        "print('cor (rand2, rand2): ', df.stat.corr('rand2', 'rand2'))\n",
        "print('cor (rand1, rand2): ', df.stat.corr('rand1', 'rand2'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpYZ1n6TnYy7",
        "outputId": "f2858b11-0178-4e33-adbd-93d95e6ca6bc"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cor (rand2, rand2):  1.0\n",
            "cor (rand1, rand2):  -0.00048485287141450097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FunçÕes definidas pelo usuário Spark SQL\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import LongType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SquareFunction\").master(\"local[*]\").getOrCreate()\n",
        "\n",
        "def square(s):\n",
        "  return s*s\n",
        "\n",
        "#Registrar UDF\n",
        "spark.udf.register(\"Square\", square, LongType())\n",
        "\n",
        "#Gerar View temporária\n",
        "spark.range(1, 10000).createOrReplaceTempView(\"udf_test\")\n",
        "\n",
        "#Run query\n",
        "spark.sql(\"SELECT id, square(id) from udf_test\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t73TpXNzu5uO",
        "outputId": "7be7ad04-aa7a-48dc-b5d3-289ce84bd017"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+\n",
            "| id|Square(id)|\n",
            "+---+----------+\n",
            "|  1|         1|\n",
            "|  2|         4|\n",
            "|  3|         9|\n",
            "|  4|        16|\n",
            "|  5|        25|\n",
            "|  6|        36|\n",
            "|  7|        49|\n",
            "|  8|        64|\n",
            "|  9|        81|\n",
            "| 10|       100|\n",
            "| 11|       121|\n",
            "| 12|       144|\n",
            "| 13|       169|\n",
            "| 14|       196|\n",
            "| 15|       225|\n",
            "| 16|       256|\n",
            "| 17|       289|\n",
            "| 18|       324|\n",
            "| 19|       361|\n",
            "| 20|       400|\n",
            "+---+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-OgXVMrRv47X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}